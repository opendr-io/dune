{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667e3b94-4d6b-4f89-b494-b8711331ec11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*pin_memory.*no accelerator.*\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53869f96-5c26-4bdd-9a5c-c7d941008c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A Notebook For Threat Hunting Using an Ensemble ML Model\n",
    "Part of the DUNE project (https://github.com/opendr-io/dune) and useful for hunting threats that are resistant to conventional detection. This notebook does ensemble model anomaly detection using fields of your choice. The examples below are for analysis of CloudTrail logs. The notebook will run fourteen models by default; there are four more that can optionally be run which tend to take more time. The notebook will score each row in the dataframe according to how many models evaluated it as an outlier. The scored anomalous events can be queried, sifted, aggregated, and sorted using the tools at the end. The notebook uses the pyod package which is on Github at: https://github.com/yzhao062/pyod and has docs here: https://pyod.readthedocs.io/en/latest/pyod.models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723c0b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\flynn\\\\.conda\\\\envs\\\\dune\\\\python.exe',\n",
       " '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]',\n",
       " ['c:\\\\Users\\\\flynn\\\\.conda\\\\envs\\\\dune',\n",
       "  'c:\\\\Users\\\\flynn\\\\.conda\\\\envs\\\\dune\\\\Lib\\\\site-packages'],\n",
       " 'z:\\\\Desktop\\\\Feb Mark Three Notebooks')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, site, os \n",
    "sys.executable, sys.version, site.getsitepackages(), os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9144dc-3cf9-481f-85a9-59d1e0f90df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pyod\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import os\n",
    "from random import randrange\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea15185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.cd import CD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.dif import DIF\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.gmm import GMM\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.inne import INNE\n",
    "from pyod.models.kde import KDE\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lmdd import LMDD\n",
    "from pyod.models.loda import LODA\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.lunar import LUNAR\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.ocsvm import OCSVM \n",
    "from pyod.models.qmcd import QMCD\n",
    "from pyod.models.rod import ROD\n",
    "from pyod.models.sod import SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760abb0-b483-4f9d-84d3-64cbb1d8a3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#variables\n",
    "pyod_out_frac=0.001\n",
    "random_state_number = 42\n",
    "rs = np.random.RandomState(random_state_number)  #random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302012e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d23f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for csv loading\n",
    "# pyod_file_path = 'cloudtrail-medium.csv'#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parquet loading\n",
    "pyod_input = pd.read_parquet(\"cloudtrail-all.parquet\")\n",
    "pyod_raw = pyod_input.copy()\n",
    "print('raw data shape and column names:')\n",
    "print(pyod_input.shape)\n",
    "print()\n",
    "print('Choose your field names:')\n",
    "pyod_input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde5eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select fields to encode\n",
    "pyod_fields_to_encode = ['sourceIPAddress', 'eventSource',  'eventName', 'userAgent', 'userIdentity.arn'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9851c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the columns you want to use for outlier detection\n",
    "pyod_columns_for_outliers = ['sourceIPAddress', 'eventSource',  'eventName', 'userAgent', 'userIdentity.arn'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2744d-b1d9-4abb-ba8b-41e061dfc9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary of classifiers\n",
    "# defines a dictionary of anomaly detection models with configurations\n",
    "# using various algorithms from libraries likely associated with\n",
    "# PyOD (Python Outlier Detection).\n",
    "clf = { \n",
    "    'CBLOF': CBLOF(contamination=pyod_out_frac, check_estimator=False, random_state=rs),\n",
    "    'IForest': IForest(contamination=pyod_out_frac,random_state=rs),\n",
    "    'KNN': KNN(contamination=pyod_out_frac),\n",
    "    'AvKNN': KNN(method='mean', contamination=pyod_out_frac),\n",
    "    'LOF':LOF(n_neighbors=35, contamination=pyod_out_frac),\n",
    "     ### added new ####\n",
    "    'ROD':ROD(contamination=pyod_out_frac),\n",
    "    'CD' : CD(contamination=pyod_out_frac),\n",
    "    'COPOD' : COPOD(contamination=pyod_out_frac),\n",
    "    'ECOD' : ECOD(contamination=pyod_out_frac),\n",
    "    'GMM' : GMM(contamination=pyod_out_frac),\n",
    "    'HBOS' : HBOS(contamination=pyod_out_frac),\n",
    "    'INNE' : INNE(contamination=pyod_out_frac),\n",
    "    'LODA' : LODA(contamination=pyod_out_frac),\n",
    "    #'QMCD' : QMCD(contamination=pyod_out_frac),\n",
    "    # Optional classifiers; these will take longer to run\n",
    "    #'OCSVM': OCSVM(contamination=pyod_out_frac), # One-class SVM 182 seconds\n",
    "    #'DIF': DIF(contamination=pyod_out_frac), # 148 seconds\n",
    "    #'KDE': KDE(contamination=pyod_out_frac), # 169 seconds\n",
    "    #'LUNAR': LUNAR(contamination=pyod_out_frac), # 227 seconds i.e. 4 minutes\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a2bb0-ac1b-4112-a677-b5e8947e04ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyod_num_samples = pyod_input.shape[0]\n",
    "pyod_fraction_of_inliers = (1. - pyod_out_frac) \n",
    "# (1 - fraction of outliers)\n",
    "pyod_num_inliers = int(pyod_fraction_of_inliers * pyod_num_samples) \n",
    "# fraction of inliers * total number of samples\n",
    "pyod_num_outliers = int(pyod_out_frac * pyod_num_samples) \n",
    "# fraction of outliers * total number of samples\n",
    "print('No. of inliers: %i' % pyod_num_inliers)\n",
    "print('No. of outliers: %i' % pyod_num_outliers)\n",
    "print('dataframe shape:')\n",
    "print(pyod_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47d93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the data type of each column to category\n",
    "# replace the original categorical values with their \n",
    "# corresponding integer codes. \n",
    "for field in pyod_fields_to_encode:\n",
    "    pyod_input[field] = pyod_input[field].astype('category')\n",
    "    pyod_input[field] = pyod_input[field].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85376b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PX = pyod_input.loc[:, pyod_columns_for_outliers].values\n",
    "print(PX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196a2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract columns from a pandas DataFrame and convert them into a NumPy array. \n",
    "for i, classifier in enumerate(clf.keys()):\n",
    "    print('Model', i + 1, ':' + classifier) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbcc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterates over a dictionary of classifiers and add a new column\n",
    "# to the dataframe for each classifier to store classifications \n",
    "# initialized to zero\n",
    "for i, (classifier_name, classifier) in enumerate(clf.items()):\n",
    "    pyod_input[str('out_'+ classifier_name)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149aac27-e6be-4243-ad2f-cbd7e801974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(f\"{str(now)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdee5a-1f0a-4bf4-b2d8-e5c1e9219736",
   "metadata": {},
   "source": [
    "### Estimator: This cell will forecast runtimes for the loaded dataframe and remove any excessively expensive models according to runtime with a default limit of 60 seconds. If your row and cardinality counts are supernumerary, consider running on batches or chunks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22963da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build PX from the working dataframe\n",
    "PX = pyod_input.loc[:, pyod_columns_for_outliers].values\n",
    "n_full = len(PX)\n",
    "\n",
    "def time_model(model, X):\n",
    "    t0 = time.perf_counter()\n",
    "    model.fit(X)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "def quick_forecast(\n",
    "    model,\n",
    "    X,\n",
    "    sample_frac=0.10,\n",
    "    sizes=(1000, 3000, 7000),\n",
    "    repeats=3,\n",
    "    seed=42,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(X)\n",
    "    sample_n = max(1000, int(n * sample_frac))\n",
    "    sample_n = min(sample_n, n)\n",
    "    idx_sample = rng.choice(n, size=sample_n, replace=False)\n",
    "    Xs = X[idx_sample]\n",
    "\n",
    "    results = []\n",
    "    for s in sizes:\n",
    "        s = min(s, len(Xs))\n",
    "        if s < 10:\n",
    "            continue\n",
    "        times = []\n",
    "        for _ in range(repeats):\n",
    "            idx = rng.choice(len(Xs), size=s, replace=False)\n",
    "            t = time_model(model, Xs[idx])\n",
    "            times.append(t)\n",
    "        t_med = float(np.median(times))\n",
    "        results.append((s, t_med))\n",
    "\n",
    "    # Enforce non-decreasing times to avoid negative exponent\n",
    "    fixed = []\n",
    "    last_t = 0.0\n",
    "    for s, t in sorted(results):\n",
    "        t = max(t, last_t)\n",
    "        fixed.append((s, t))\n",
    "        last_t = t\n",
    "\n",
    "    if len(fixed) < 2:\n",
    "        return 0.0, fixed, 0.0, 0.0\n",
    "\n",
    "    # Fit power law t = a * n^b on log-log\n",
    "    ns = np.array([p[0] for p in fixed], dtype=float)\n",
    "    ts = np.array([p[1] for p in fixed], dtype=float)\n",
    "    logn = np.log(ns)\n",
    "    logt = np.log(ts + 1e-9)  # avoid log(0)\n",
    "    b, loga = np.polyfit(logn, logt, 1)\n",
    "    b = max(b, 0.0)  # clamp negative exponent\n",
    "    a = np.exp(loga)\n",
    "    t_pred = a * (n_full ** b)\n",
    "    return t_pred, fixed, a, b\n",
    "\n",
    "clf_filtered = dict(clf)\n",
    "max_seconds = 60\n",
    "excluded = []\n",
    "predicted_times = {}\n",
    "\n",
    "for name, model in list(clf.items()):\n",
    "    t_pred, results, a, b = quick_forecast(model, PX)\n",
    "    predicted_times[name] = t_pred\n",
    "    print(f\"{name}: samples={results} | a={a:.3e}, b={b:.3f} | predicted={t_pred:.1f}s\")\n",
    "    if t_pred > max_seconds:\n",
    "        print(f\"  -> removing {name} (>{max_seconds}s)\")\n",
    "        excluded.append(name)\n",
    "        clf_filtered.pop(name, None)\n",
    "\n",
    "print(\"Excluded:\", excluded)\n",
    "print(\"Estimation Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model runner mark three - more efficient and cleaner version - tested on 300k row dataframes to run most models under 60s\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "np.random.seed(42)\n",
    "algo_failed = []\n",
    "\n",
    "# Pre-create output columns once\n",
    "for classifier_name in clf_filtered.keys():\n",
    "    pyod_input[f'out_{classifier_name}'] = np.nan\n",
    "\n",
    "for i, (classifier_name, classifier) in enumerate(clf_filtered.items()):\n",
    "    begin = time.perf_counter()\n",
    "    now = datetime.datetime.now()\n",
    "    print(f\"{now}: #{i + 1} fitting {classifier_name}\")\n",
    "    try:\n",
    "        classifier.fit(PX)\n",
    "        y_pred = classifier.predict(PX)\n",
    "        pyod_input[f'out_{classifier_name}'] = y_pred\n",
    "    except Exception as e:\n",
    "        print(f\"{now}: Error in classifier {classifier_name} {e}\")\n",
    "        algo_failed.append(classifier_name)\n",
    "\n",
    "    sec = time.perf_counter() - begin\n",
    "    now = datetime.datetime.now()\n",
    "    print(f\"{now}: Total runtime of {classifier_name} is {sec} seconds\")\n",
    "\n",
    "print(f\"Total cell time: {time.perf_counter() - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5b77f-31b3-470a-8608-1f218a53987d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "print('time check:')\n",
    "print(f\"{str(now)}\")\n",
    "if not algo_failed:\n",
    "    print(\"✓ All anomaly detection algorithms ran successfully.\")\n",
    "else:\n",
    "    print(\"⚠ Some algorithms failed:\")\n",
    "    for name, err in algo_failed.items():\n",
    "        print(f\"  - {name}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter and process results from the models\n",
    "out_algos = []\n",
    "for c in pyod_input.columns:\n",
    "    if c.startswith('out_'):\n",
    "        out_algos.append(c)\n",
    "\n",
    "#calculating the rank which is the sum of the number of algorithms \n",
    "# which detected a row as an outlier\n",
    "pyod_df_small =  pyod_input[out_algos].copy()\n",
    "pyod_df_small['rank'] = pyod_df_small.iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "# analyze the distribution of values in the 'rank' column of the pyod_df_small DataFrame.\n",
    "print('total number of rank values:', (pyod_df_small['rank'].nunique()))\n",
    "print('breakdown of row count by rank scores:')\n",
    "print(pyod_df_small['rank'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6d5f8-1196-4542-a25b-815f1a1df182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_rows_indices = pyod_df_small.query('rank > 0')\n",
    "# create an dataframe with the output of the models\n",
    "\n",
    "# identify all outlier-vote columns\n",
    "out_algos = [c for c in pyod_input.columns if c.startswith('out_')]\n",
    "\n",
    "# sum votes across algorithms → rank\n",
    "pyod_input['rank'] = pyod_input[out_algos].sum(axis=1)\n",
    "pyod_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10504528-16c0-4118-9ec3-f63a01662cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyod_row_indices = pyod_input.index[pyod_input['rank'] > 0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24406d-9684-4816-9aaa-d4df68b2e81a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pyod_row_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dd68b-2264-4a05-bcf2-2aea1ae258c8",
   "metadata": {},
   "source": [
    "#### Statistics: Heatmap and Correlations Between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65aac5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(pyod_df_small[out_algos].corr(), annot=True)\n",
    "# pearson correlation between different algorithms - this tells us \n",
    "# which algorithms are returning similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434591f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyod_df_small[out_algos].corr(method='pearson')\n",
    "#This tells us which algorithms are similar i.e. detecting the same row as outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21f22-2366-4286-9763-f765c56182ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All model vote columns\n",
    "out_algos = [c for c in pyod_input.columns if c.startswith('out_')]\n",
    "\n",
    "# Sum of \"outlier\" votes per row\n",
    "pyod_input['rank'] = pyod_input[out_algos].sum(axis=1)\n",
    "pyod_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55779f9c-8a5e-43b7-9823-173416513488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scored anomalies in the encoded dataframe\n",
    "pyod_anomalies_scored = pyod_input[pyod_input['rank'] > 0][['eventID', 'rank'] ].copy()\n",
    "\n",
    "# list of GUIDs that were flagged\n",
    "pyod_anomalous_eventids = pyod_anomalies_scored['eventID'].unique()\n",
    "\n",
    "# pull original CloudTrail rows based on GUIDs\n",
    "#pyod_raw = pd.read_parquet(\"cloudtrail-300.parquet\")\n",
    "pyod_output = pyod_raw[pyod_raw['eventID'].isin(pyod_anomalous_eventids)].copy()\n",
    "\n",
    "# optional: keep rank and votes attached by merging back on eventid\n",
    "pyod_output = pyod_output.merge(\n",
    "    pyod_input[['eventID', 'rank'] + out_algos],\n",
    "    on='eventID',\n",
    "    how='left',\n",
    "    suffixes=('', '_score')\n",
    ")\n",
    "pd.set_option('display.max_rows', 135)\n",
    "pd.set_option('display.max_rows', 135)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pyod_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d0193-4ce1-4b64-8944-d47dba512f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check that the event IDs match between the raw and scored data rows\n",
    "pyod_ts_check = (\n",
    "    pyod_raw[['eventID', 'eventTime']]\n",
    "    .merge(\n",
    "        pyod_output[['eventID', 'eventTime']],\n",
    "        on='eventID',\n",
    "        how='inner',\n",
    "        suffixes=('_pyod_raw', '_ddf')\n",
    "    )\n",
    ")\n",
    "pyod_ts_check['eventTime_match'] = (\n",
    "    pyod_ts_check['eventTime_pyod_raw'] == pyod_ts_check['eventTime_ddf']\n",
    ")\n",
    "pyod_ts_check['eventTime_match'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83978bcf-e335-4afe-805b-b08bf00c5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyod_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562f204-eded-48e1-9a07-ba2696dfda01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_algos = [c for c in pyod_input.columns if c.startswith('out_')]\n",
    "pyod_input['rank'] = pyod_input[out_algos].sum(axis=1)\n",
    "rank_lookup = pyod_input.loc[pyod_input['rank'] > 0, ['eventID', 'rank']]\n",
    "pyod_output = pyod_raw.merge(\n",
    "    rank_lookup,\n",
    "    on='eventID',\n",
    "    how='inner'\n",
    ")\n",
    "assert pyod_output['rank'].gt(0).all()\n",
    "pyod_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932de02-7f52-45f2-b124-2526b681b51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyod_output.to_csv('pyod_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040fd24-e52a-488c-a32e-20bea828281c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "# simple beep\n",
    "display(Audio(data=np.sin(2*np.pi*440*np.linspace(0, 1, 44100)), rate=44100, autoplay=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099799f1-9e3a-4774-bdb4-d09831d3619b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
